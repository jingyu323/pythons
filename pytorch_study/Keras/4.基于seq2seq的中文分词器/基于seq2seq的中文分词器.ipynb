{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence-to-Sequence模型  \n",
    "<center><img src=\"seq2seq.jpg\" alt=\"FAO\" width=\"500\"></center> \n",
    "1.可用于机器翻译  \n",
    "2.文章摘要  \n",
    "3.对话机器人  \n",
    "4.中文分词  \n",
    "......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# \n",
    "text = open('msr_train_10.txt').read()\n",
    "text = text.split('\\n')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "len(text)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# {B:begin, M:middle, E:end, S:single}，分别代表每个状态代表的是该字在词语中的位置，\n",
    "# B代表该字是词语中的起始字，M代表是词语中的中间字，E代表是词语中的结束字，S则代表是单字成词\n",
    "text"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 设置参数\n",
    "# 词向量长度\n",
    "word_size = 128\n",
    "# 设置最长的一句话为32个字\n",
    "maxlen = 32\n",
    "# 批次大小\n",
    "batch_size = 1024\n",
    "\n",
    "# 根据符号分句\n",
    "text = u''.join(text)\n",
    "text = re.split(u'[，。！？、]/[bems]', text)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "len(text)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "text"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 训练集数据\n",
    "data = []\n",
    "# 标签\n",
    "label = []\n",
    "\n",
    "\n",
    "# 得到所有的数据和标签\n",
    "def get_data(s):\n",
    "    s = re.findall('(.)/(.)', s)\n",
    "    if s:\n",
    "        s = np.array(s)\n",
    "        # 返回数据和标签，0为数据，1为标签\n",
    "        return list(s[:, 0]), list(s[:, 1])\n",
    "\n",
    "\n",
    "for s in text:\n",
    "    d = get_data(s)\n",
    "    if d:\n",
    "        data.append(d[0])\n",
    "        label.append(d[1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "test = re.findall('(.)/(.)', '你/s  只/b  有/e  把/s  事/b  情/e  做/b  好/e')\n",
    "test"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 定义一个dataframe存放数据和标签\n",
    "d = pd.DataFrame(index=range(len(data)))\n",
    "d['data'] = data\n",
    "d['label'] = label\n",
    "# 提取data长度小于等于maxlen的数据\n",
    "d = d[d['data'].apply(len) <= maxlen]\n",
    "# 重新排列index\n",
    "d.index = range(len(d))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "source": "d",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#统计所有字，给每个字编号\n",
    "chars = []\n",
    "for i in data:\n",
    "    chars.extend(i)\n",
    "\n",
    "chars = pd.Series(chars).value_counts()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "chars"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "chars[:] = range(1, len(chars) + 1)\n",
    "chars"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#生成适合模型输入的格式\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# 定义标签所对应的编号\n",
    "tag = pd.Series({'s': 0, 'b': 1, 'm': 2, 'e': 3, 'x': 4})\n",
    "\n",
    "\n",
    "# # 把中文变成编号，再补0\n",
    "# d['x'] = d['data'].apply(lambda x: np.array(list(chars[x])+[0]*(maxlen-len(x))))\n",
    "# # 把标签变成编号，再补0\n",
    "# d['y'] = d['label'].apply(lambda x: np.array(list(map(lambda y:np_utils.to_categorical(y,5), tag[x].reshape((-1,1))))+[np.array([[0,0,0,0,1]])]*(maxlen-len(x))))\n",
    "\n",
    "\n",
    "def data_helper(x):\n",
    "    x = list(chars[x]) + [0] * (maxlen - len(x))\n",
    "    return np.array(x)\n",
    "\n",
    "\n",
    "def label_helper(x):\n",
    "    x = list(map(lambda y: np_utils.to_categorical(y, 5), tag[x].reshape((-1, 1))))\n",
    "    x = x + [np.array([[0, 0, 0, 0, 1]])] * (maxlen - len(x))\n",
    "    return np.array(x)\n",
    "\n",
    "\n",
    "d['x'] = d['data'].apply(data_helper)\n",
    "d['y'] = d['label'].apply(label_helper)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "d['data'][0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "d['x'][0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "d['label'][0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "d['y'][0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"lstm1.png\" alt=\"FAO\" width=\"500\"></center> \n",
    "<center><img src=\"lstm2.png\" alt=\"FAO\" width=\"500\"></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 设计模型\n",
    "from keras.layers import Dense, Embedding, LSTM, TimeDistributed, Input, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "\n",
    "sequence = Input(shape=(maxlen,), dtype='int32')\n",
    "# 词汇数，词向量长度，输入的序列长度，是否忽略0值\n",
    "embedded = Embedding(len(chars) + 1, word_size, input_length=maxlen, mask_zero=True)(sequence)\n",
    "# 双向RNN包装器\n",
    "blstm = Bidirectional(LSTM(64, return_sequences=True), merge_mode='sum')(embedded)\n",
    "# 该包装器可以把一个层应用到输入的每一个时间步上\n",
    "output = TimeDistributed(Dense(5, activation='softmax'))(blstm)\n",
    "# 定义模型输出输出\n",
    "model = Model(inputs=sequence, outputs=output)\n",
    "# 定义代价函数，优化器\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# print(np.array(list(d['x'])).shape)\n",
    "# print(np.array(list(d['y'])).reshape((-1,maxlen,5)).shape)\n",
    "# model.fit(np.array(list(d['x'])), np.array(list(d['y'])).reshape((-1,maxlen,5)), batch_size=batch_size, epochs=20)\n",
    "# model.save('seq2seq.h5')\n",
    "\n",
    "print(\"load model\")\n",
    "model = load_model('seq2seq.h5')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 做预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 设置最长的一句话为32个字\n",
    "maxlen = 32\n",
    "\n",
    "# 使用全数据\n",
    "text = open('msr_train.txt').read()\n",
    "text = text.split('\\n')\n",
    "\n",
    "# 根据符号分句\n",
    "text = u''.join(text)\n",
    "text = re.split(u'[，。！？、]/[bems]', text)\n",
    "\n",
    "# 训练集数据\n",
    "data = []\n",
    "# 标签\n",
    "label = []\n",
    "\n",
    "\n",
    "# 得到所有的数据和标签\n",
    "def get_data(s):\n",
    "    s = re.findall('(.)/(.)', s)\n",
    "    if s:\n",
    "        s = np.array(s)\n",
    "        # 返回数据和标签，0为数据，1为标签\n",
    "        return list(s[:, 0]), list(s[:, 1])\n",
    "\n",
    "\n",
    "for s in text:\n",
    "    d = get_data(s)\n",
    "    if d:\n",
    "        data.append(d[0])\n",
    "        label.append(d[1])\n",
    "\n",
    "# 定义一个dataframe存放数据和标签\n",
    "d = pd.DataFrame(index=range(len(data)))\n",
    "d['data'] = data\n",
    "d['label'] = label\n",
    "# 提取data长度小于等于maxlen的数据\n",
    "d = d[d['data'].apply(len) <= maxlen]\n",
    "# 重新排列index\n",
    "d.index = range(len(d))\n",
    "\n",
    "#统计所有字，给每个字编号\n",
    "chars = []\n",
    "for i in data:\n",
    "    chars.extend(i)\n",
    "\n",
    "chars = pd.Series(chars).value_counts()\n",
    "chars[:] = range(1, len(chars) + 1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "print(\"load model\")\n",
    "model = load_model('seq2seq.h5')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 统计状态转移\n",
    "dict_label = {}\n",
    "for label in d['label']:\n",
    "    for i in range(len(label) - 1):\n",
    "        tag = label[i] + label[i + 1]\n",
    "        dict_label[tag] = dict_label.get(tag, 0) + 1\n",
    "print(dict_label)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 计算状态转移总次数\n",
    "sum_num = 0\n",
    "for value in dict_label.values():\n",
    "    sum_num = sum_num + value\n",
    "sum_num"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 计算状态转移概率\n",
    "p_ss = dict_label['ss'] / sum_num\n",
    "p_sb = dict_label['sb'] / sum_num\n",
    "p_bm = dict_label['bm'] / sum_num\n",
    "p_be = dict_label['be'] / sum_num\n",
    "p_mm = dict_label['mm'] / sum_num\n",
    "p_me = dict_label['me'] / sum_num\n",
    "p_es = dict_label['es'] / sum_num\n",
    "p_eb = dict_label['eb'] / sum_num"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 维特比算法，维特比算法是一种动态规划算法用于寻找最有可能产生观测事件序列的-维特比路径\n",
    "\n",
    "# tag = pd.Series({'s':0, 'b':1, 'm':2, 'e':3, 'x':4})\n",
    "\n",
    "# 00 = ss = 1\n",
    "# 01 = sb = 1\n",
    "# 02 = sm = 0\n",
    "# 03 = se = 0\n",
    "# 10 = bs = 0\n",
    "# 11 = bb = 0\n",
    "# 12 = bm = 1\n",
    "# 13 = be = 1\n",
    "# 20 = ms = 0\n",
    "# 21 = mb = 0\n",
    "# 22 = mm = 1\n",
    "# 23 = me = 1\n",
    "# 30 = es = 1\n",
    "# 31 = eb = 1\n",
    "# 32 = em = 0\n",
    "# 33 = ee = 0\n",
    "\n",
    "# 定义状态转移矩阵\n",
    "transfer = [[p_ss, p_sb, 0, 0],\n",
    "            [0, 0, p_bm, p_be],\n",
    "            [0, 0, p_mm, p_me],\n",
    "            [p_es, p_eb, 0, 0]]\n",
    "\n",
    "# # 定义状态转移矩阵\n",
    "# transfer = [[1,1,0,0],\n",
    "#             [0,0,1,1],\n",
    "#             [0,0,1,1],\n",
    "#             [1,1,0,0]]\n",
    "\n",
    "# 根据符号断句\n",
    "cuts = re.compile(u'([\\da-zA-Z ]+)|[。，、？！\\.\\?,!]')\n",
    "\n",
    "\n",
    "# 预测分词\n",
    "def predict(sentence):\n",
    "    # 如果句子大于最大长度，只取maxlen个词\n",
    "    if len(sentence) > maxlen:\n",
    "        sentence = sentence[:maxlen]\n",
    "\n",
    "    # 预测结果，先把句子变成编号的形式，如果出现生僻字就填充0，然后给句子补0直到maxlen的长度。预测得到的结果只保留跟句子有效数据相同的长度\n",
    "    result = \\\n",
    "    model.predict(np.array([list(chars[list(sentence)].fillna(0).astype(int)) + [0] * (maxlen - len(sentence))]))[0][\n",
    "    :len(sentence)]\n",
    "\n",
    "    # 存放最终结果\n",
    "    y = []\n",
    "    # 存放临时概率值\n",
    "    prob = []\n",
    "    # 计算最大转移概率\n",
    "    # 首先计算第1个字和第2个字,统计16种情况的概率\n",
    "    # result[0][j]第1个词的标签概率\n",
    "    # result[1][k]第2个词的标签概率\n",
    "    # transfer[j][k]对应的转移概率矩阵的概率\n",
    "    for j in range(4):\n",
    "        for k in range(4):\n",
    "            # 第1个词为标签j的概率*第2个词为标签k的概率*jk的转移概率\n",
    "            prob.append(result[0][j] * result[1][k] * transfer[j][k])\n",
    "\n",
    "    # 计算前一个词的的标签\n",
    "    word1 = np.argmax(prob) // 4\n",
    "    # 计算后一个词的标签\n",
    "    word2 = np.argmax(prob) % 4\n",
    "    # 保存结果\n",
    "    y.append(word1)\n",
    "    y.append(word2)\n",
    "    # 从第2个字开始\n",
    "    for i in range(1, len(sentence) - 1):\n",
    "        # 存放临时概率值\n",
    "        prob = []\n",
    "        # 计算前一个字和后一个字的所有转移概率\n",
    "        for j in range(4):\n",
    "            # 前一个字的标签已知为word2\n",
    "            prob.append(result[i][word2] * result[i + 1][j] * transfer[word2][j])\n",
    "        # 计算后一个字的标签\n",
    "        word2 = np.argmax(prob) % 4\n",
    "        # 保存结果\n",
    "        y.append(word2)\n",
    "\n",
    "    # 分词\n",
    "    words = []\n",
    "    for i in range(len(sentence)):\n",
    "        # 如果标签为s或b，append到结果的list中\n",
    "        if y[i] in [0, 1]:\n",
    "            words.append(sentence[i])\n",
    "        else:\n",
    "            # 如果标签为m或e，在list最后一个元素中追加内容\n",
    "            words[-1] += sentence[i]\n",
    "    return words\n",
    "\n",
    "\n",
    "# 分句\n",
    "def cut_word(s):\n",
    "    result = []\n",
    "    # 指针设置为0\n",
    "    j = 0\n",
    "    # 根据符号断句\n",
    "    for i in cuts.finditer(s):\n",
    "        # 对符号前的部分分词\n",
    "        result.extend(predict(s[j:i.start()]))\n",
    "        # 加入符号\n",
    "        result.append(s[i.start():i.end()])\n",
    "        # 移动指针到符号后面\n",
    "        j = i.end()\n",
    "    # 对最后的部分进行分词\n",
    "    result.extend(predict(s[j:]))\n",
    "    return result"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "cut_word('基于seq2seq的中文分词器')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "cut_word('人们常说生活是一部教科书')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "cut_word('广义相对论是描写物质间引力相互作用的理论')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "cut_word('我爱北京天安门，天安门上太阳升')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "model.predict(\n",
    "    np.array([list(chars[list('今天天气很好')].fillna(0).astype(int)) + [0] * (maxlen - len('今天天气很好'))]))[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3 align = \"center\">欢迎大家关注我的公众号，或者加我的微信与我交流。</h3>\n",
    "<center><img src=\"wx.png\" alt=\"FAO\" width=\"300\"></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
