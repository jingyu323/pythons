# 优化算法和优化器

## 随机梯度下降法 (SGD)
随机梯度下降（Stochastic Gradient Descent, SGD）是一种常用的优化算法，主要用于训练机器学习模型，尤其是神经网络。是训练优化神经网络的常用方法。

它的基本思想是基于单个样本或小批量样本来更新模型参数，从而加速优化过程。

### 简介

SGD的基本思想是通过**逐个样本或小批量样本来更新模型参数**，而不是使用整个数据集。这种方法大大提高了**计算效率**，特别是在处理大规模数据集时。

优劣分析
优点：

计算效率高：每次更新只使用一个样本或一个小批量样本，计算速度快，适合大规模数据集。
在线学习：SGD可以很容易地应用于在线学习，即通过连续获取数据流实时更新模型。
更好的模型泛化性：由于参数更新有一定的随机性，SGD有助于避免陷入局部最优解，从而获得更好的模型泛化性。
缺点：

收敛不稳定：由于每次只使用一个样本计算梯度，参数更新路径非常不稳定，可能导致优化过程中的振荡。
需要调整学习率：学习率的选择非常关键且敏感，通常需要仔细调整以获得最佳效果。
局部解问题：尽管随机性有助于避免陷入局部解，但它不总是能够找到全局最优解。

## Adam算法



## RMSPROP算法



## adagrad算法



## softmax算法  多分类

多选项问题使用

categorical_crossentropy

sparse_categorical_crossentropy   计算softmax 交叉熵

## sigmoid算法 逻辑回归

解决最终结果是true false的问题



## 目标检测经典模型算法