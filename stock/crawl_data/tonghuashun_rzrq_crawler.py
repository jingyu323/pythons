from operator import indexOf

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from datetime import datetime



class RZRQCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'http://data.10jqka.com.cn/market/rzrq/',
            'Host': 'data.10jqka.com.cn'
        }
        self.base_url = "http://data.10jqka.com.cn/market/rzrq/"

    def crawl_rzrq_data(self):
        """çˆ¬å–èèµ„èåˆ¸æ•°æ®"""
        try:
            response = requests.get(self.base_url, headers=self.headers, timeout=10)
            response.encoding = 'gbk'

            if response.status_code == 200:
                # print(response.text)
                soup = BeautifulSoup(response.text, 'lxml')

                tdiv = soup.find('div', attrs={'class': 'page-table'})

                # æŸ¥æ‰¾æ•°æ®è¡¨æ ¼
                table = tdiv.find('table', attrs={'class': 'm-table'})

                # tables = tdiv.find_all('table')
                # for table in tables:
                #     print(table)

                # print(table)

                data_list = []
                if table:
                    tbody = table.find('tbody') # è·³è¿‡è¡¨å¤´
                    rows = tbody.find_all('tr') # è·³è¿‡è¡¨å¤´
                    for row in rows:
                        cols = row.find_all('td')
                        if len(cols) > 0:
                            stock_data = {
                                'è‚¡ç¥¨ä»£ç ': cols[1].text.strip(),
                                'è‚¡ç¥¨åç§°': cols[2].text.strip(),
                                'èèµ„ä½™é¢':   cols[3].text.strip().replace("äº¿","")  if  "äº¿" in cols[3].text.strip() else cols[3].text.strip().replace("ä¸‡",""),
                                'èèµ„ä¹°å…¥é¢':  cols[4].text.strip().replace("äº¿","")  if  "äº¿" in cols[4].text.strip() else cols[4].text.strip().replace("ä¸‡",""),
                                'èèµ„å¿è¿˜é¢':   cols[5].text.strip().replace("äº¿","")  if  "äº¿" in cols[5].text.strip() else cols[5].text.strip().replace("ä¸‡",""),
                                'èèµ„å‡€ä¹°å…¥é¢':  cols[6].text.strip().replace("äº¿","")  if  "äº¿" in cols[6].text.strip() else cols[6].text.strip().replace("ä¸‡",""),
                                'èåˆ¸ä½™é¢': cols[7].text.strip() ,
                                'èåˆ¸å–å‡ºé‡': cols[8].text.strip() ,
                                'èåˆ¸å‡€ä¹°å…¥': cols[9].text.strip() ,
                                'èåˆ¸å‡€å–å‡º': cols[10].text.strip() ,
                                'èèµ„èåˆ¸ä½™é¢':  cols[11].text.strip().replace("äº¿","")  if  "äº¿" in cols[11].text.strip() else cols[11].text.strip().replace("ä¸‡","")
                            }
                            data_list.append(stock_data)

                return pd.DataFrame(data_list)

        except Exception as e:
            print(f"çˆ¬å–å¤±è´¥: {e}")
            return None

    def crawl_multiple_pages(self, max_pages=5):
        """åˆ†é¡µçˆ¬å–æ•°æ®"""
        base_url = "http://data.10jqka.com.cn/market/rzrq/board/{}/field/rzrqye/order/desc/page/{}/"
        all_data = []

        for page in range(1, max_pages + 1):
            url = base_url.format("rzrq", page)
            try:
                response = requests.get(url, headers=self.headers, timeout=10)
                response.encoding = 'utf-8'

                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    table = soup.find('table', {'class': 'm-table'})

                    if table:
                        rows = table.find_all('tr')[1:]
                        for row in rows:
                            cols = row.find_all('td')
                            if len(cols) > 0:
                                stock_data = {
                                    'è‚¡ç¥¨ä»£ç ': cols[1].text.strip(),
                                    'è‚¡ç¥¨åç§°': cols[2].text.strip(),
                                    'èèµ„ä½™é¢': cols[3].text.strip(),
                                    'èèµ„ä¹°å…¥é¢': cols[4].text.strip(),
                                    'èèµ„å¿è¿˜é¢': cols[5].text.strip(),
                                    'èèµ„å‡€ä¹°å…¥é¢': cols[6].text.strip(),
                                    'èåˆ¸ä½™é¢': cols[7].text.strip(),
                                    'èåˆ¸å–å‡ºé‡': cols[8].text.strip(),
                                    'èåˆ¸å‡€ä¹°å…¥': cols[9].text.strip(),
                                    'èåˆ¸å‡€å–å‡º': cols[10].text.strip(),
                                    'èèµ„èåˆ¸ä½™é¢': cols[11].text.strip()
                                }

                                all_data.append(stock_data)

                time.sleep(1)  # æ·»åŠ å»¶æ—¶é¿å…è¢«å°

            except Exception as e:
                print(f"ç¬¬{page}é¡µçˆ¬å–å¤±è´¥: {e}")
                continue

        return pd.DataFrame(all_data)

    def clean_data(self, df):
        """æ•°æ®æ¸…æ´—"""
        if df is None or df.empty:
            return df

        # å»é™¤ç©ºå€¼
        df = df.dropna()

        # è½¬æ¢æ•°æ®ç±»å‹
        for col in ['èèµ„ä½™é¢', 'èèµ„ä¹°å…¥é¢', 'èåˆ¸ä½™é¢', 'èåˆ¸å–å‡ºé‡', 'èèµ„èåˆ¸ä½™é¢']:
            if col in df.columns:
                df[col] = df[col].str.replace(',', '').astype(float)

        return df

    def analyze_data(self, df):
        """æ•°æ®åˆ†æ"""
        if df is None or df.empty:
            return None
        # èèµ„ä½™é¢æ’åå‰å
        top_10_rz = df.nlargest(10, 'èèµ„ä½™é¢')
        print(top_10_rz)
        print("====")

        # èåˆ¸ä½™é¢æ’åå‰å
        top_10_rq = df.nlargest(10, 'èåˆ¸ä½™é¢')

        # æ€»ä½™é¢ç»Ÿè®¡
        total_balance = df['èèµ„èåˆ¸ä½™é¢'].sum() if 'èèµ„èåˆ¸ä½™é¢' in df.columns else 0

        return {
            'total_balance': total_balance,
            'top_rz': top_10_rz,
            'top_rq': top_10_rq
        }

    def save_data(self, df, filename=None):
        """ä¿å­˜æ•°æ®"""
        if df is None or df.empty:
            print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return

        if filename is None:
            filename = f"èèµ„èåˆ¸æ•°æ®_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

        # ä¿å­˜ä¸ºCSV
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"æ•°æ®å·²ä¿å­˜è‡³: {filename}")

    def daily_crawl(self):
        """æ¯æ—¥å®šæ—¶çˆ¬å–"""
        print(f"{datetime.now()} å¼€å§‹æ‰§è¡Œæ¯æ—¥æ•°æ®çˆ¬å–")
        df = self.crawl_rzrq_data()
        if df is not None:
            df = self.clean_data(df)
            self.save_data(df)
            print("æ¯æ—¥çˆ¬å–ä»»åŠ¡å®Œæˆ")
        else:
            print("æ•°æ®çˆ¬å–å¤±è´¥")


def main():
    """ä¸»å‡½æ•°"""
    crawler = RZRQCrawler()

    print("ğŸš€ å¼€å§‹çˆ¬å–åŒèŠ±é¡ºèèµ„èåˆ¸æ•°æ®...")

    # çˆ¬å–æ•°æ®
    df = crawler.crawl_rzrq_data()

    if df is not None:
        print(f"âœ… æˆåŠŸçˆ¬å– {len(df)} æ¡èèµ„èåˆ¸æ•°æ®")
        print("\nå‰5æ¡æ•°æ®é¢„è§ˆ:")
        print(df.head())

        # æ•°æ®æ¸…æ´—
        df_cleaned = crawler.clean_data(df)

        # æ•°æ®åˆ†æ
        analysis_result = crawler.analyze_data(df_cleaned)
        if analysis_result:
            print(f"\nğŸ“Š èèµ„èåˆ¸æ€»ä½™é¢: {analysis_result['total_balance']:,.2f} å…ƒ")
            print("\nğŸ“ˆ èèµ„ä½™é¢å‰åè‚¡ç¥¨:")
            print(analysis_result['top_rz'][['è‚¡ç¥¨åç§°', 'èèµ„ä½™é¢']].to_string(index=False))
            print("\nğŸ“‰ èåˆ¸ä½™é¢å‰åè‚¡ç¥¨:")
            print(analysis_result['top_rq'][['è‚¡ç¥¨åç§°', 'èåˆ¸ä½™é¢']].to_string(index=False))

        # ä¿å­˜æ•°æ®
        crawler.save_data(df_cleaned)

        # æ¼”ç¤ºåˆ†é¡µçˆ¬å–åŠŸèƒ½
        print("\nğŸ”„ æ¼”ç¤ºåˆ†é¡µçˆ¬å–åŠŸèƒ½(å‰3é¡µ)...")
        df_multi = crawler.crawl_multiple_pages(max_pages=3)
        if not df_multi.empty:
            print(f"åˆ†é¡µçˆ¬å–å…±è·å¾— {len(df_multi)} æ¡æ•°æ®")
            crawler.save_data(df_multi, f"åˆ†é¡µèèµ„èåˆ¸æ•°æ®_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")

    else:
        print("âŒ æ•°æ®çˆ¬å–å¤±è´¥")


if __name__ == "__main__":
    main()
